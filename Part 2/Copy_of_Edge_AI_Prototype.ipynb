{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcAPQYctCAEt",
        "outputId": "d84eeb03-9a81-4a44-a71d-c8db5b520784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 60 files belonging to 3 classes.\n",
            "Using 48 files for training.\n",
            "Found 60 files belonging to 3 classes.\n",
            "Using 12 files for validation.\n",
            "Classes: ['metal', 'paper', 'plastic']\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 167ms/step - accuracy: 0.2673 - loss: 1.2195 - val_accuracy: 0.7500 - val_loss: 0.5341\n",
            "Epoch 2/5\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 189ms/step - accuracy: 0.9065 - loss: 0.4062 - val_accuracy: 1.0000 - val_loss: 0.0577\n",
            "Epoch 3/5\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 205ms/step - accuracy: 1.0000 - loss: 0.0460 - val_accuracy: 1.0000 - val_loss: 0.0044\n",
            "Epoch 4/5\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 3.5851e-05\n",
            "Epoch 5/5\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 3.5336e-05 - val_accuracy: 1.0000 - val_loss: 3.3974e-06\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.1000 - loss: 1.1799\n",
            "Saved artifact at 'saved_model'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='keras_tensor_134')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  140688635948368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140688635944528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140688635939728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140688635940112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140688635950864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  140688635951632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "✅ TensorFlow Lite model created successfully!\n",
            "Output from TFLite model inference: [[2.3680516e-06 9.9999762e-01 3.7079220e-12]]\n"
          ]
        }
      ],
      "source": [
        "#Step 1 Setup Sample Dataset (Fake Mini Dataset for Testing)\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Create folders\n",
        "categories = ['plastic', 'paper', 'metal']\n",
        "base_dir = 'recyclables_dataset'\n",
        "\n",
        "if os.path.exists(base_dir):\n",
        "    shutil.rmtree(base_dir)\n",
        "\n",
        "for category in categories:\n",
        "    os.makedirs(os.path.join(base_dir, category))\n",
        "\n",
        "# Generate sample images (colored shapes to simulate classes)\n",
        "def create_sample_image(color, path):\n",
        "    img = Image.new('RGB', (128, 128), color=color)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    draw.rectangle([30, 30, 90, 90], fill='white')\n",
        "    img.save(path)\n",
        "\n",
        "colors = {'plastic': 'blue', 'paper': 'green', 'metal': 'gray'}\n",
        "\n",
        "# Save 20 fake images per category\n",
        "for category in categories:\n",
        "    for i in range(20):\n",
        "        img_path = os.path.join(base_dir, category, f'{category}_{i}.jpg')\n",
        "        create_sample_image(colors[category], img_path)\n",
        "\n",
        "\n",
        "#Step 2: Load Data & Train Model\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "batch_size = 8\n",
        "img_size = (128, 128)\n",
        "\n",
        "train_ds = image_dataset_from_directory(\n",
        "    base_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_ds = image_dataset_from_directory(\n",
        "    base_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Build a lightweight CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Rescaling(1./255, input_shape=(128, 128, 3)),\n",
        "    layers.Conv2D(16, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(len(class_names), activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=5)\n",
        "\n",
        "\n",
        "#Step 3: Convert to Tensor Flow Lite\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# 1. Create dummy data\n",
        "num_classes = 3\n",
        "input_shape = (128, 128, 3)\n",
        "x_train = np.random.random((10, *input_shape)).astype(np.float32)\n",
        "y_train = np.random.randint(num_classes, size=(10,))\n",
        "\n",
        "# 2. Build simple model\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    layers.Conv2D(16, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 3. Train the model\n",
        "model.fit(x_train, y_train, epochs=1)\n",
        "\n",
        "# 4. Export model as TensorFlow SavedModel (new way in TF 2.13+)\n",
        "model.export('saved_model')\n",
        "\n",
        "# 5. Convert SavedModel to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# 6. Save TFLite model file\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"✅ TensorFlow Lite model created successfully!\")\n",
        "\n",
        "\n",
        "#Step 4: Test TFLite Model on One Image\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Assume model.tflite was already created and saved properly in Colab's working directory\n",
        "\n",
        "tflite_model_path = 'model.tflite'  # Make sure this matches the saved filename\n",
        "\n",
        "# Check if the TFLite model file exists\n",
        "import os\n",
        "if not os.path.exists(tflite_model_path):\n",
        "    raise FileNotFoundError(f\"TFLite model file not found at: {tflite_model_path}\")\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Prepare a dummy input that matches the model input shape and type\n",
        "input_shape = input_details[0]['shape']\n",
        "input_dtype = input_details[0]['dtype']\n",
        "\n",
        "# Create a dummy input tensor (random or zeros)\n",
        "dummy_input = np.random.random_sample(input_shape).astype(input_dtype)\n",
        "\n",
        "# Set the tensor to the interpreter\n",
        "interpreter.set_tensor(input_details[0]['index'], dummy_input)\n",
        "\n",
        "# Run inference\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the output\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Output from TFLite model inference:\", output_data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohDdidjpDbDl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}